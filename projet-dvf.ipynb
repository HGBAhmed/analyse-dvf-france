{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Analyse brivement: \n",
    "* La base DVF enregistre les données des transactions immobilières de tous les communes en frances métropoles (hors Alsace et Moselle) ainsi que les département d'Outre-Mer (hors Mayotte). \n",
    "* À partir de ce jeux de données, nous pourons faire des analyses pour la valeur foncières en france selon différentes critères (typologie de biens, la nature de mutation,créer des outils cartographique interactive avec la valeur foncière par chaque communes et par le département (par exemple, comme les outils cartographiques que MeilleursAgents a utilisé, nous pourrons faire la carte de prix par m² des bureaux/commerces/entrepôts par communes en France). \n",
    "* Nous pourrons savoir aussi à partir de ce jeu de données l'évoution selon le temps ( entre 2014 et 2020 ) pour chaque type de biens.\n",
    "* Le point faible de ce jeu, c'est qu'il n'y a pas mal des informations manquantes (comme les caractéristiques du bien, le nombre d'étage, ascenseur, année de construction ...) qui nous permet de faire une estimation la valeur du bien en utilisant la méthode hédonique et en prendre en compte l'impacte de la localisation spatial du bien.\n",
    "* De plus,le jeu de données contient aussi beaucoup des données manquants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Préparation des données:\n",
    "\n",
    "Pour une étude quantitatives, la partie préparation des données joue un rôle très important. Plus on travail bien sur cette partie, plus on a les chances pour avoir les résultats pertinences dans les étapes analyses et modélisation, prédiction les données! Pour cette étape, nous avons réalisé les techniques comme:\n",
    "               - Créer les dataset brutes et vérifier les données dans ce dataset.\n",
    "               - Traitement des données manquantes et remplacer les données manquantes par méthode l'imputation.\n",
    "               - Changement les types de certaines variables et choisir les variables importants pour analyser!\n",
    "               - L'analyse exploratoire des données pour voir la rélation entre la valeur foncière et d'autres variables dans le jeu de données. Pour le traitement des outliers (données abérrantes) nous ne pouvons pas faire en raison des biens immobiliers dans ce jeu de données sont hétérogènes et de plus, pour la partie modélisation et la classification des données nous travaillons sur une autre jeu de données. C'est pour cette raison, je ne mets pas la partie traitement des outliers ici. \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importer les packages python\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import descartes\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Définir la chemin pour le jeu de données\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ici, nous travaillons sur les transactions en 2020 comme une exemple\n",
    "df_full20 = pd.read_csv('/kaggle/input/france-geolocated-dvf-land-values/2020-full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#La base brute pour l'année 2020\n",
    "df_full20.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Vérifier le taux des données manquantes dans la base brutes\n",
    "df_full20.isnull().sum()/len(df_full20)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ici, nous constatons que la base brute contient beaucoup de variables qui ont un taux de plus de 90% des données manquantes. Donc, pour ces variables, nous avons exclué dans notre jeu de données. \n",
    "* Pour les autres variables qui ont un taux de moins de 40%, nous utilisons la méthode de l'imputation de données pour remplacer les données manquantes ( par exemple, on peut remplacer les données manquantes dans la valeur foncière par la valeur moyenne ou par le médian afin de ne pas changer la distribution de cette variables!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_final = df_full20.drop(['numero_disposition',\n",
    "                          'ancien_code_commune',\n",
    "                          'adresse_suffixe',\n",
    "                           'ancien_nom_commune',\n",
    "                           'ancien_id_parcelle',\n",
    "                           'numero_volume',\n",
    "                           'lot1_numero',\n",
    "                           'lot1_surface_carrez',\n",
    "                           'lot2_numero',\n",
    "                           'lot2_surface_carrez',\n",
    "                           'lot3_numero',\n",
    "                           'lot3_surface_carrez',\n",
    "                           'lot4_numero',\n",
    "                           'lot4_surface_carrez',\n",
    "                           'lot5_numero',\n",
    "                           'lot5_surface_carrez',\n",
    "                           'code_nature_culture',\n",
    "                           'nature_culture',\n",
    "                           'code_nature_culture_speciale',\n",
    "                           'nature_culture_speciale'],axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Changer le types de variables\n",
    "df_final['code_postal'] = df_final['code_postal'].astype('object')\n",
    "df_final['code_type_local'] = df_final['code_type_local'].astype('object')\n",
    "df_final['type_local'] = df_final['type_local'].astype('object')\n",
    "df_final['nombre_pieces_principales'] = df_final['nombre_pieces_principales'].astype('object')\n",
    "df_final['code_departement'] = df_final['code_departement'].astype('object')\n",
    "df_final['code_commune'] = df_final['code_commune'].astype('object')\n",
    "df_final['nombre_lots'] = df_final['nombre_lots'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse exploratoire pour certains variables importants!\n",
    "\n",
    "* L'objectif de cette étape est de savoir la distribution des variables importants dans cette base et de voir la relation entre les variables explicatives dans ce jeu avec le prix de l'immobilière (la valeur foncière).\n",
    "* Nous commençons par l'étude de la distribution et la répartition des variables qualitatives et quantitatives dans ce jeu de données et ensuite nous allons vérifier la rélation (la corrélation) entre les variables à la fin de cette étape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_final['type_local'].value_counts(dropna = False, normalize = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Créer les graphiques!\n",
    "plt.figure(figsize=(10,10))\n",
    "labels = ['Donées Manquantes', 'Maison', 'Appartement', 'Dépendance', 'Local industriel. commercial ou assimilé']\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fffc52']\n",
    "area = [41, 21.4, 18, 12.9, 0.37]\n",
    "#area = [91.93, 6.88, 0.82, 0.28,0.056]\n",
    "explode = (0.05,0.05,0.1,0.1,0.1)\n",
    "plt.pie(area, labels=labels, explode=explode, colors= colors, startangle=65, autopct='%1.1f%%',shadow='True')\n",
    "plt.title(label= 'Répartition de la variable type_local')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ici, on voit que pour les transactions immobilières en 2020, les types de biens d'ici sont répartie différement!\n",
    "                   - 43.8% les transactions n'a pas été saisi par le type de biens.\n",
    "                   - 22.8% les transantions concerne les maisons en France.\n",
    "                   - 19.2% les transantions concerne les appartements en France.\n",
    "                   - Les transactions des bureaux/commerces/entrepôts sont juste 0.4% par rapport la volume totale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_final['code_type_local'].value_counts(dropna = False, normalize = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Répartition de la nature de mutation du bien!\n",
    "df_final['nature_mutation'].value_counts(dropna = False, normalize = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualiser la répartition de la nature de mutation\n",
    "plt.figure(figsize=(10,10))\n",
    "labels = ['Vente', 'VEFA', 'Echange', 'Vente Terrain', 'Adjudication']\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fffc52']\n",
    "area = [92, 6.8, 0.82, 0.28, 0.56]\n",
    "#area = [91.93, 6.88, 0.82, 0.28,0.056]\n",
    "explode = (0.05,0.05,0.2,0.3,0.4)\n",
    "plt.pie(area, labels=labels, explode=explode, colors= colors, startangle=65, autopct='%1.1f%%',shadow='True')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous trouvons que 91% les tranctions immobilières en 2020 pour tous les régions en France sont les ventes immobilières. La vente en VEFA est juste 6.8% et les restes sont moins de 2% pour les autres types de transactions.\n",
    "Du coup, nous serons travailler sur la Vente dans ce jeu de données (en excluant tous les autres types de transactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Traiter les données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# L'imputation des données manquantes\n",
    "df_final['adresse_numero'] = df_full20['adresse_numero'].fillna(\"None\")\n",
    "df_final['adresse_nom_voie'] = df_full20['adresse_nom_voie'].fillna(\"None\")\n",
    "df_final['adresse_code_voie'] = df_full20['adresse_code_voie'].fillna(\"None\")\n",
    "df_final['code_type_local'] = df_full20['code_type_local'].fillna(\"None\")\n",
    "df_final['type_local'] = df_full20['type_local'].fillna(\"None\")\n",
    "df_final['valeur_fonciere'] = df_final['valeur_fonciere'].transform(lambda x: x.fillna(x.mean()))\n",
    "df_final['surface_reelle_bati'] = df_full20['surface_reelle_bati'].fillna(0)\n",
    "df_final['nombre_pieces_principales'] = df_full20['nombre_pieces_principales'].fillna(0)\n",
    "df_final['longitude'] = df_full20['longitude'].fillna(0)\n",
    "df_final['latitude'] = df_full20['latitude'].fillna(0)\n",
    "df_final['surface_reelle_bati'] = df_full20['surface_reelle_bati'].fillna(0)\n",
    "df_final['surface_terrain'] = df_final['surface_terrain'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Verifier s'il y a des données manquantes dans la base\n",
    "df_final.isnull().sum()/len(df_full20)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#3) Filtrer les données pour avoir une base qui contient que les données pour la local commercial, industriel et assimilé\n",
    "filtered_df = df_final[(df_final[\"nature_mutation\"] == 'Vente')  &  (df_final[\"code_type_local\"] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filtered_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Vérifier le nombre des variables quantitatives et qualitatives dans notre jeu de donnée\n",
    "num_vars = filtered_df.dtypes[filtered_df.dtypes != \"object\"].index\n",
    "cat_vars = filtered_df.dtypes[filtered_df.dtypes == \"object\"].index\n",
    "\n",
    "print(\"Nombres des variables quantitatives: \", len(num_vars))\n",
    "print(\"Nombres des variables qualitatives: \", len(cat_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Créer une fonction pour calculer la corrélation entre les variables quantitatives\n",
    "def spearman(frame, features):\n",
    "    spr = pd.DataFrame()\n",
    "    spr['feature'] = features\n",
    "    spr['spearman'] = [frame[f].corr(frame['valeur_fonciere'], 'spearman') for f in features]\n",
    "    spr = spr.sort_values('spearman')\n",
    "    plt.figure(figsize=(6, 0.25*len(features)))\n",
    "    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n",
    "    \n",
    "features = num_vars\n",
    "spearman(filtered_df, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Pour analyser la corrélation entre les variables dans notre jeu de données, nous utlisons la test spearman et pearson pour savoir la rélation entre les variables quantitatives.\n",
    "**** Ici, nous trouvons que la valeur foncière est corrélé possitivement avec la surface du terrain et la surface reelle du bâtiment. C'est à dire plus la surface est grande, plus la valeur est cher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Matrice de corrélation entre les variables (heatmap)\n",
    "corrmat = filtered_df.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pour étudier la corrélation entre une variable quantitative et une variable qualitative, nous pouvons utiliser le box-plot pour voir la rélation entre eux. Ici, j'étudie la relation entre le valeur foncière et le code_departement. L'idée est pour voir si la localisation géographique du bien a une impact sur le prix de transaction immobilière. Les commentaires se trouvent en dessous du graph.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Box Plot\n",
    "data = pd.concat([filtered_df['valeur_fonciere'], filtered_df['code_departement']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(30, 30))\n",
    "fig = sns.boxplot(x=filtered_df['code_departement'], y=filtered_df['valeur_fonciere'], data=data)\n",
    "fig.axis(ymin=0, ymax=65000000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Grâce à ce graphique, je vois que la valeur foncière est forte corrélé avec la localisation du bien. Par exemple, pour les déparments dans la régions Ile-de-France, on voit la différent très claire par rapport les autres départements. Les biens dans ces départements a été vendu plus chers par rapport les autres régions de la France. On peut conclure que, pour les prix immobilière en France, la régions Ile-de-France est un cas spécialisé en terme de prix, et la volume de transaction!\n",
    "\n",
    "* Pour les autres départments en France, on trouve que les département Alpes-maritimes - Nice, Bouches-du-Rhône - Marseille, Haute-garonne - Toulouse, Loire-atlantique - Nantes, Nord - Lille, Gironde - Bordeaux, Rhône - Lyon, les prix sont toujours plus haut que les autres régions de la France (sauf Ile-de-France). \n",
    "* Cela est compréhensible, parce que ce sont les plus grands métropoles françaises ou les marché immobiliers sont tendu. Par exemple pour le marché des bureaux, lyon prend la 2 ème position juste après le Grand Paris! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "sns.distplot(filtered_df['valeur_fonciere'])\n",
    "\n",
    "print(\"Skewness: %f\" % filtered_df['valeur_fonciere'].skew())\n",
    "print(\"Kurtosis: %f\" % filtered_df['valeur_fonciere'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici que la distribution de la valeur fonciere est sysmétrie positivement. Cela indique une distribution décalée à gauche de la médiane, et donc une queue de distribution étalée vers la droite. Dans les études de prix immobilier (notament pour la modélisation et l'estimation prix immobilière), normalement nous transformons le prix en logarithme (log_normale) et de plus, on va s'intéresser au prix par m2 plutôt que le prix total d'une transaction. \n",
    "\n",
    "Par ailleurs, on trouves que le type de bien ici sont redondances (il comprend les bureaux, les commerces, les entrepôt) dont cela donne des valeurs foncieres très hétérogènes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Ici, nous transformons la valeur fonciere en log pour voir la distribution de cette variable: \n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "y = np.log(filtered_df['valeur_fonciere'])\n",
    "sns.distplot(y, fit=norm);\n",
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pour la classification les bureaux, commerces et entrepots à partir de la variable type_local je vois que:\n",
    " - Avec seulements les données disponibles dans ce jeu de données, pour classfier les types des bâtiments nous pourrons utiliser certaines méthodes comme la K-Nearest Neighboor ou le Clustering mais personnellement, je trouves que les résultats ne seronts pas signigicatives vu que nous n'avons pas beaucoup des informations concernant les caractéristiques du bien pour faire la classification.\n",
    " - De l'autre côté, pour chaque bien on a les données géolocalisée (variables coordinates), c'est une information très important. À partir de la variable cordinnates, nous pouvons trouver les images \"street view\" pour chaque bâtiment avec l'API Google Street View Static. Et puis, nous pouvons collecter tous les images \"façade\" de chaque bâtiments et créer une dataset contient ques les images.\n",
    " - Avec ce dataset, nous pouvons faire ensuite une modèle Réseaux Neuronal Convolutif pour faire la classification des bâtiments à partir de ces images (Il existe aussi d'autres techniques classification comme la SVM, RandomForest Classifier, Regression Logistique Multinomial, etc.).\n",
    " - Mais vu que l'API de google n'est pas gratuit pour collecter les images. Donc, j'ai essayé de faire une exemple en collectant une base des images qui contients ques les photos de façades pour les batiments en Etats-Unis. Ce dataset contient 6600 images (en trois classe: industrial, office building, retail) pour les données apprentisage et 845 images pour la validation (3 class: industrial, office building, retail). "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1114559,
     "sourceId": 1872116,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30055,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
